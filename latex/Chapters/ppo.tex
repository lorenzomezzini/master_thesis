\section{Reinforcement Learning}

Reinforcement learning is learning what to do to maximize a numerical reward. A discovery process where the learner is not told what action to take, but instead must discover which action yields the most reward by trying them. In the most interesting cases actions might affect not only the immediate reward but also future situations and all the subsequent rewards. These two characteristics -- Trial-and-error search and delayed reward -- are the two most important distinguishing features of reinforcement learning.

Reinforcement learning differs from \textit{supervised learning} since training is not based on an external dataset of labeled examples, where each situation (observation) is labelled with the correct action to perform (often identify a category). RL, although one might erroneously think the opposite, is also different from \textit{unsupervised learning}. The main objective for unsupervised learning is to find hidden structures in an unlabeled dataset, whereas RL's main objective is to maximize a reward signal. 

\subsection{Elements of Reinforcement Learning}

\section{Finite Markov Decision process (MDPs)}

Finite Markov decision processes are a class of problems that formalize subsequent decision making, where not only the influence of the action is exerted on immediate reward but also on those in the future. MDP's are suited for RL since these are models that frame the process of learning through repeated interaction between an agent (decision maker), and an environment (ruled by fixed state transition function).

More specifically an agent is asked to take an action \( a_t \in \mathcal{A} \) at every time step \( t = 0,1,...\, \). To do so the agent is provided with an observation of the current environment's state \( s_t \in \mathcal{S} \) and a reward \( r_t \in \) from the previously performed action.

More specifically at every point in time \( t = 0,1,...\, \) the agent is provided with an observation of the current environment's state \( s_t \in \mathcal{S} \) and a reward \( r_t \in \) from the previously performed action. From this informations the agent is asked to choose an action \( a_t \in \mathcal{A}(s) \). \textcolor{red}{aggiungi nota: perchè A è in funzione di s} Afterwards the environment update it's state following a transition distribution \( \mathcal{T}(s_{t+1}|a_t,s_t) \)\textcolor{red}{controlla come si scrive per bene} and generates a new state \( s_{t+1} \) and a numerical reward \( r_{t+1} \in \mathcal{R} \subset \mathbb{R} \). This process is reproduced every subsequent timestep, this concatenation of interaction is a MDP.

\textcolor{red}{aggiungi roba dal capitolo 3.2 in poi}

\section{}

\section{Policy gradient Methods}  
\textcolor{red}{Chapter 13 RL: an into}


Policy gradient methods are a set of parameterized policy that can select actions without the use of a value function


\section{Proximal Policy Optimization Algorithms}


Proximal policy optimization algorithms (PPOs) are a family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent.
